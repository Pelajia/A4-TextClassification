# -*- coding: utf-8 -*-
"""Untitled40.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f_cxC2_koWYbwUeYOyGZLdA6VRY5zy0D
"""

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, Flatten
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.datasets import imdb

# Classe de base pour la classification de texte
# Cette classe définit les méthodes de base pour la gestion des données d'entraînement et de test
class BaseTextClassifier:
    def __init__(self, X, y):
        # Division des données en ensembles d'entraînement et de test
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    def train(self):
        # Méthode à implémenter dans les classes dérivées
        raise NotImplementedError("Train method not implemented")

    def evaluate(self):
        # Méthode à implémenter dans les classes dérivées
        raise NotImplementedError("Evaluate method not implemented")

# 1. Bag-of-Words + Régression Logistique
class BagOfWordsLogistic(BaseTextClassifier):
    def __init__(self, X, y):
        super().__init__(X, y)
        # Initialisation du vectoriseur Bag-of-Words
        self.vectorizer = CountVectorizer()
        # Initialisation du modèle de régression logistique
        self.model = LogisticRegression()

    def train(self):
        # Transformation des données d'entraînement en vecteurs Bag-of-Words
        X_train_vec = self.vectorizer.fit_transform(self.X_train)
        # Entraînement du modèle
        self.model.fit(X_train_vec, self.y_train)

    def evaluate(self):
        # Transformation des données de test en vecteurs Bag-of-Words
        X_test_vec = self.vectorizer.transform(self.X_test)
        # Prédiction des étiquettes
        y_pred = self.model.predict(X_test_vec)
        # Calcul et retour de la précision
        return accuracy_score(self.y_test, y_pred)

# 2. TF-IDF + Régression Logistique
class TfidfLogistic(BaseTextClassifier):
    def __init__(self, X, y):
        super().__init__(X, y)
        # Initialisation du vectoriseur TF-IDF
        self.vectorizer = TfidfVectorizer()
        # Initialisation du modèle de régression logistique
        self.model = LogisticRegression()

    def train(self):
        # Transformation des données d'entraînement en vecteurs TF-IDF
        X_train_vec = self.vectorizer.fit_transform(self.X_train)
        # Entraînement du modèle
        self.model.fit(X_train_vec, self.y_train)

    def evaluate(self):
        # Transformation des données de test en vecteurs TF-IDF
        X_test_vec = self.vectorizer.transform(self.X_test)
        # Prédiction des étiquettes
        y_pred = self.model.predict(X_test_vec)
        # Calcul et retour de la précision
        return accuracy_score(self.y_test, y_pred)

# 3. N-grams (2,3,5) + Régression Logistique
class NgramLogistic(BaseTextClassifier):
    def __init__(self, X, y, ngram_range=(2, 5)):
        super().__init__(X, y)
        # Initialisation du vectoriseur N-grams avec la plage spécifiée
        self.vectorizer = CountVectorizer(ngram_range=ngram_range)
        # Initialisation du modèle de régression logistique
        self.model = LogisticRegression()

    def train(self):
        # Transformation des données d'entraînement en vecteurs N-grams
        X_train_vec = self.vectorizer.fit_transform(self.X_train)
        # Entraînement du modèle
        self.model.fit(X_train_vec, self.y_train)

    def evaluate(self):
        # Transformation des données de test en vecteurs N-grams
        X_test_vec = self.vectorizer.transform(self.X_test)
        # Prédiction des étiquettes
        y_pred = self.model.predict(X_test_vec)
        # Calcul et retour de la précision
        return accuracy_score(self.y_test, y_pred)

# 4. Word Embedding + MLP
class WordEmbeddingMLP(BaseTextClassifier):
    def __init__(self, X, y, max_words=30000, max_len=100):
        super().__init__(X, y)
        # Initialisation du tokenizer pour créer les embeddings
        self.tokenizer = Tokenizer(num_words=max_words)
        # Longueur maximale des séquences
        self.max_len = max_len
        # Initialisation du modèle MLP
        self.model = Sequential([
            Embedding(max_words, 128, input_length=max_len),
            Flatten(),
            Dense(64, activation='relu'),
            Dense(1, activation='sigmoid')
        ])
        # Compilation du modèle
        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    def train(self):
        # Ajustement du tokenizer sur les données d'entraînement
        self.tokenizer.fit_on_texts(self.X_train)
        # Conversion des textes en séquences numériques
        X_train_seq = pad_sequences(self.tokenizer.texts_to_sequences(self.X_train), maxlen=self.max_len)
        # Entraînement du modèle avec les séquences
        self.model.fit(X_train_seq, np.array(self.y_train), epochs=5, batch_size=32)

    def evaluate(self):
        # Conversion des textes de test en séquences numériques
        X_test_seq = pad_sequences(self.tokenizer.texts_to_sequences(self.X_test), maxlen=self.max_len)
        # Évaluation du modèle sur les données de test
        loss, accuracy = self.model.evaluate(X_test_seq, np.array(self.y_test))
        # Retour de la précision
        return accuracy

# 5. LSTM
class LSTMClassifier(BaseTextClassifier):
    def __init__(self, X, y, max_words=30000, max_len=100):
        super().__init__(X, y)
        # Initialisation du tokenizer pour les séquences
        self.tokenizer = Tokenizer(num_words=max_words)
        # Longueur maximale des séquences
        self.max_len = max_len
        # Initialisation du modèle LSTM
        self.model = Sequential([
            Embedding(max_words, 128, input_length=max_len),
            LSTM(64),
            Dense(1, activation='sigmoid')
        ])
        # Compilation du modèle
        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    def train(self):
        # Ajustement du tokenizer sur les données d'entraînement
        self.tokenizer.fit_on_texts(self.X_train)
        # Conversion des textes en séquences numériques
        X_train_seq = pad_sequences(self.tokenizer.texts_to_sequences(self.X_train), maxlen=self.max_len)
        # Entraînement du modèle avec les séquences
        self.model.fit(X_train_seq, np.array(self.y_train), epochs=5, batch_size=32)

    def evaluate(self):
        # Conversion des textes de test en séquences numériques
        X_test_seq = pad_sequences(self.tokenizer.texts_to_sequences(self.X_test), maxlen=self.max_len)
        # Évaluation du modèle sur les données de test
        loss, accuracy = self.model.evaluate(X_test_seq, np.array(self.y_test))
        # Retour de la précision
        return accuracy

# Exemple d'utilisation
if __name__ == "__main__":
    # Charger le jeu de données IMDB
    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=30000)

    # Décoder les indices en texte lisible (facultatif)
    word_index = imdb.get_word_index()
    reverse_word_index = {value: key for key, value in word_index.items()}
    X_train_text = [" ".join([reverse_word_index.get(i - 3, "?") for i in review]) for review in X_train[:5000]]
    X_test_text = [" ".join([reverse_word_index.get(i - 3, "?") for i in review]) for review in X_test[:1000]]

    # Sélection des données pour les classifieurs
    X, y = X_train_text, y_train[:5000]

    # Liste des classifieurs à tester
    classifiers = [
        BagOfWordsLogistic(X, y),
        TfidfLogistic(X, y),
        NgramLogistic(X, y),
        WordEmbeddingMLP(X, y),
        LSTMClassifier(X, y)
    ]

    # Entraînement et évaluation pour chaque classifieur
    for clf in classifiers:
        clf.train()
        print(f"Précision de {clf.__class__.__name__} : {clf.evaluate():.2f}")